{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oySXv39fDki5"
      ],
      "authorship_tag": "ABX9TyMHjA8zDBCaGfJ+UNGjNHNP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samtru99/PDF-Chatbot/blob/main/pdf_chatbot_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "oySXv39fDki5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain pinecone-client tiktoken openai faiss-cpu PyPDF2"
      ],
      "metadata": {
        "id": "UQ8UViRrDYw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LangChain\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains import LLMChain, HypotheticalDocumentEmbedder\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "import langchain\n",
        "#OpenAI\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "#Others\n",
        "import os\n",
        "import re\n",
        "import pinecone\n",
        "import tiktoken\n",
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "SoiGT1KICZ-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1 - Extracing data from the PDF"
      ],
      "metadata": {
        "id": "xyKdLu-Z_pBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Link to Google Drive***"
      ],
      "metadata": {
        "id": "ovmlulZmDxTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"content/gdrive/My Drive\"\n",
        "\n",
        "reader = PdfReader('/Path/Here/To/PDF')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Q6JVxED-Jw",
        "outputId": "8473af9e-b14d-46e9-b126-c3f017b7b48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Extract raw Text from PDF***"
      ],
      "metadata": {
        "id": "keyU2wgNEWKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = ''\n",
        "for i, page in enumerate(reader.pages):\n",
        "  text = page.extract_text()\n",
        "  if text:\n",
        "    raw_text += text"
      ],
      "metadata": {
        "id": "w3jcn-Y-EVpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2 - Data Preparation/Preprocessing"
      ],
      "metadata": {
        "id": "MAMmWlNQE6Qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer function to calculate the total amount of tokens required to process"
      ],
      "metadata": {
        "id": "m63zLHTUFFAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "#Create a length function\n",
        "def tiktoken_len(text):\n",
        "  tokens = tokenizer.encode(\n",
        "      text,\n",
        "      disallowed_special=()\n",
        "  )\n",
        "  return len(tokens)"
      ],
      "metadata": {
        "id": "g_J0HWCvFUQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilize a text splitter function to break down the raw text into chunks"
      ],
      "metadata": {
        "id": "4MR4ZTy4FZPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 400,\n",
        "    chunk_overlap = 20,\n",
        "    length_function = tiktoken_len,\n",
        "    separators = ['\\n\\n', '\\n', ' ', '']\n",
        ")"
      ],
      "metadata": {
        "id": "qRfxz2BxHMEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Break down the raw text into chunks of less than 400 tokens"
      ],
      "metadata": {
        "id": "ossMukn1HX5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "jo9Nayh8IiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean up text to reduce the amount of tokens needed"
      ],
      "metadata": {
        "id": "N9xxjUQ6IxL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_func(text):\n",
        "    clean_text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
        "    clean_text = re.sub(r'[ \\t]+', ' ', clean_text)\n",
        "    clean_text = re.sub(r'[\\n\\n]', '', clean_text)\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "wsSS9sy8Iwwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3 - Embed Text and Store Vectors in Pinecone"
      ],
      "metadata": {
        "id": "otHl3f9WKGvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilize OpenAI Embedding"
      ],
      "metadata": {
        "id": "TbHTgkyBKkU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    openai_api_key = \"ENTER YOUR KEY HERE\"\n",
        ")"
      ],
      "metadata": {
        "id": "vWYJ0uUiKw3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Pinecone Database"
      ],
      "metadata": {
        "id": "CH39Z-aCK4ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(\n",
        "    api_key = 'ENTER YOUR KEY HERE',\n",
        "    environment = 'gcp-starter'\n",
        ")\n",
        "\n",
        "if 'csedb' not in pinecone.list_indexes():\n",
        "  pinecone.create_index('csedb', dimension = 1536)\n",
        "\n",
        "index = pinecone.Index('csedb')\n"
      ],
      "metadata": {
        "id": "kM_kTgsHK-sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embed and store all chunks\n",
        "\n"
      ],
      "metadata": {
        "id": "wo_FUF-qLKmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(chunks)):\n",
        "  clean_txt = clean_text_func(chunks[i])\n",
        "  em_vector = model.embed_documents(clean_txt)\n",
        "  meta = [{\n",
        "      'text': clean_txt\n",
        "  }]\n",
        "  id = f\"chunk_{i}\"\n",
        "  index.upsert(\n",
        "      vectors=[\n",
        "          {\n",
        "              'id': id,\n",
        "              'values': em_vector,\n",
        "              'metadata': {'text': clean_txt}\n",
        "          }\n",
        "      ]\n",
        "  )"
      ],
      "metadata": {
        "id": "FUCiTbmlLRd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4 - Query User Questions"
      ],
      "metadata": {
        "id": "c00Las_fMdt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize ChatGPT and Pinecone"
      ],
      "metadata": {
        "id": "zGEVBahIN6Ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_field = \"text\"\n",
        "vectorstore = Pinecone(\n",
        "    index, model, text_field\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=\"ENTER YOUR KEY HERE\",\n",
        "    model=\"gpt-3.5-turbo\"\n",
        ")"
      ],
      "metadata": {
        "id": "nZh0OK4KOBM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize HyDE to be used in semantic searches"
      ],
      "metadata": {
        "id": "xgqTuuKbO-R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyDE_embedding = HypotheticalDocumentEmbedder.from_llm(\n",
        "    chat, model, prompt_key=\"web_search\"\n",
        ")"
      ],
      "metadata": {
        "id": "lLBLR0QnQjsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def end_convo():\n",
        "  return False\n",
        "\n",
        "\n",
        "'''\n",
        "  Main portion of the program\n",
        "'''\n",
        "def continue_convo(messages):\n",
        "\n",
        "  def augmented(query: str):\n",
        "    hyDE_ans = hyDE_embedding.embed_query(query)\n",
        "    results = index.query(top_k=3,vector = hyDE_ans, include_metadata=True)\n",
        "    source_knowledge = \"\\n\".join([x['metadata']['text'] for x in results['matches']])\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "    Contexts:\n",
        "    {source_knowledge}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "\n",
        "    return augmented_prompt\n",
        "  question = input(\"Enter in Question\")\n",
        "  prompt = HumanMessage(\n",
        "      content = augmented(question)\n",
        "  )\n",
        "  messages.append(prompt)\n",
        "  ai_response = chat(messages)\n",
        "  print(f\"AI: \\n{ai_response.content}\")\n",
        "  messages.append(ai_response)\n",
        "  return True\n",
        "\n",
        "def new_convo():\n",
        "  messages.clear()\n",
        "  messages.append(SystemMessage(content=\"You are a helpful assistant\"))\n",
        "  return True\n",
        "\n",
        "def default_case():\n",
        "  print(\"Invalid Input\")\n",
        "  return True\n",
        "\n",
        "\n",
        "convo = True\n",
        "switch_dict = {\n",
        "      'E': end_convo,\n",
        "      'C': continue_convo,\n",
        "      'N': new_convo\n",
        "  }\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant\")\n",
        "]\n",
        "while convo:\n",
        "  if len(messages) == 1:\n",
        "    print(\"Please ask a question to begin the conversation\")\n",
        "    continue_convo(messages)\n",
        "  else:\n",
        "    user_input = input(f\"End Conversation (E)\\nContinue Conversation(C)\\nNew Conversation(N)\")\n",
        "    action = switch_dict.get(user_input, default_case)\n",
        "\n",
        "    if action == continue_convo:\n",
        "      convo = action(messages)\n",
        "    else:\n",
        "      convo = action()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"convo over\")"
      ],
      "metadata": {
        "id": "XX-Lmn0XRpzs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}